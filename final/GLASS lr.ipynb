{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8816521-29a2-4adf-84eb-a87657a3da87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import scipy\n",
    "from matplotlib.colors import ListedColormap\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9c9f48b-2df2-4a6f-a1dd-a85a7699f85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('glass.data', header=None, delimiter=',', dtype=float)\n",
    "df = df.drop([0,1,2,3,4,5,6,8], axis=1)\n",
    "df = df[df[10].isin([1.0, 2.0])]\n",
    "df[10] = df[10].replace({1.0: 1, 2.0: -1})\n",
    "df[10] = df[10].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f99e2ab2-ee0f-4c74-9f35-8a2e8be20364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(146, 2)\n",
      "(146, 1)\n",
      "[-0.27868438 -0.69899332 -1.        ]\n",
      "(116, 2)\n",
      "(116,)\n",
      "(30, 2)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "X = df.iloc[:, :-1].values  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "Y = df.iloc[:, -1].values  \n",
    "Y = Y.reshape(-1, 1).astype(float) \n",
    "#Y[Y == 0] = -1 \n",
    "X_and_Y = np.hstack((X, Y))     # Stack them together for shuffling.\n",
    "np.random.seed(1)               # Set the random seed.\n",
    "np.random.shuffle(X_and_Y)      # Shuffle the data points in X_and_Y array\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_and_Y[0])\n",
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:2]\n",
    "Y_shuffled = X_and_Y[:,2]\n",
    "\n",
    "split_index = int(0.8 * len(X_shuffled))  # 80% for training\n",
    "\n",
    "X_train = X_shuffled[:split_index]\n",
    "Y_train = Y_shuffled[:split_index]\n",
    "X_test = X_shuffled[split_index:]\n",
    "Y_test = Y_shuffled[split_index:]   \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdcc1b2-9a7f-4be3-903f-add6518670cf",
   "metadata": {},
   "source": [
    "# Logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5992505-f81c-4a5f-b471-f532f73045c5",
   "metadata": {},
   "source": [
    "# 20 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd7a1a2-52b9-4cba-9988-41bb9714eb93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(29, 2)\n",
      "(29,)\n",
      "(117, 2)\n",
      "(117,)\n"
     ]
    }
   ],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:2]\n",
    "Y_shuffled = X_and_Y[:,2]\n",
    "\n",
    "split_index = int(0.2 * len(X_shuffled))  # 20% for training\n",
    "\n",
    "X_train = X_shuffled[:split_index]\n",
    "Y_train = Y_shuffled[:split_index]\n",
    "X_test = X_shuffled[split_index:]\n",
    "Y_test = Y_shuffled[split_index:]   \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ffd4fae-6efa-4c20-a15e-011a9cdff13f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Sigmoid function: sigmoid(z) = 1/(1 + e^(-z))\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "# Judge function: 1(a != b).\n",
    "def judge(a, b):\n",
    "    \"\"\"\n",
    "    Judge function: 1(a != b).\n",
    "    Return 1 if a != b, otherwise return 0.\n",
    "    \"\"\"\n",
    "    if a != b:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def f_logistic(x, W, b):\n",
    "    \"\"\"\n",
    "    Logistic classifier: f(x, W, b)\n",
    "    This function should return -1 or 1.\n",
    "\n",
    "    x should be a 2-dimensional vector, \n",
    "    W should be a 2-dimensional vector,\n",
    "    b should be a scalar.\n",
    "    \"\"\"\n",
    "    if sigmoid(W.T.dot(x)+b) >= 0.5:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "# Calculate error given feature vectors X and labels Y.\n",
    "def calc_error(X, Y, W, b):\n",
    "    e = 0\n",
    "    n = len(X)\n",
    "    for (xi, yi) in zip(X, Y):\n",
    "        # Hint: Use judge() and f_logistic()\n",
    "        predicted_label = f_logistic(xi, W, b)\n",
    "        e+=judge(yi, predicted_label)\n",
    "    \n",
    "    # Hint: remember we want the average error.\n",
    "    e = 1.0*e/n\n",
    "    return e\n",
    "# Gradient of L(W, b) with respect to W and b.\n",
    "def grad_L_W_b(X, Y, W, b):\n",
    "    P = sigmoid(Y*(np.dot(X, W)+b))\n",
    "    delta = np.ones_like(Y)-P\n",
    "    grad_W = -np.dot(X.T,delta*Y)\n",
    "    grad_b = -np.dot(np.ones(delta.shape).T, delta * Y) #dotting ones is the same as summing everything, wow\n",
    "    return grad_W, grad_b\n",
    "# Loss L(W, b).\n",
    "def L_W_b(X, Y, W, b):\n",
    "    P = sigmoid(Y*(np.dot(X, W)+b))\n",
    "    loss = -np.dot(np.ones_like(Y).T, np.log(P))\n",
    "\n",
    "    return loss\n",
    "def logistic_regression(X_train, Y_train):\n",
    "    # Some settings.\n",
    "    losses = []           # Error history.\n",
    "    learning_rate = 0.001 # Learning rate, fixed\n",
    "    iterations    = 10000 # Iteration number, fixed\n",
    "\n",
    "    # Gradient descent algorithm for logistic regression.\n",
    "    # Step 1. Initialize the parameters W, b.\n",
    "    W      = np.zeros(2)  # Weight.\n",
    "    b      = 0.0          # Bias.\n",
    "\n",
    "    # Logistic regression learning algorithm.\n",
    "    for i in range(iterations):\n",
    "        # Step 2. Compute the partial derivatives.\n",
    "        grad_W, grad_b = grad_L_W_b(X_train, Y_train, W, b)\n",
    "        # Step 3. Update the parameters.\n",
    "        W = W-learning_rate*grad_W\n",
    "        b = b-learning_rate*grad_b\n",
    "        \n",
    "        # Track the training losses.\n",
    "        loss = L_W_b(X_train, Y_train, W, b)\n",
    "        losses.append(loss)\n",
    "\n",
    "    return W, b, losses\n",
    "#W,b = logistic_regression(X_train, Y_train)\n",
    "def cross_validate_logistic_regression(X, Y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    validation_errors = []  # To store validation errors for each fold\n",
    "    models = []  # To store W, b, and losses for each fold\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        Y_train, Y_val = Y[train_idx], Y[val_idx]\n",
    "\n",
    "        # Train the model on the training data\n",
    "        W, b, losses = logistic_regression(X_train, Y_train)\n",
    "        \n",
    "        # Evaluate on the validation set\n",
    "        error = calc_error(X_val, Y_val, W, b)\n",
    "        validation_errors.append(error)\n",
    "        \n",
    "        # Store the model for this fold\n",
    "        models.append((W, b, losses))\n",
    "    \n",
    "    # Average validation error across folds\n",
    "    avg_validation_error = np.mean(validation_errors)\n",
    "    return avg_validation_error, models, validation_errors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0953bc9f-536b-43c8-86b9-51fc0fd3fc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.001, Average Cross-Validation Score: 0.5533333333333333\n",
      "C: 0.01, Average Cross-Validation Score: 0.5533333333333333\n",
      "C: 0.1, Average Cross-Validation Score: 0.4533333333333333\n",
      "C: 1, Average Cross-Validation Score: 0.42000000000000004\n",
      "C: 10, Average Cross-Validation Score: 0.4533333333333333\n",
      "C: 100, Average Cross-Validation Score: 0.4533333333333333\n",
      "Best C: 0.001 with score: 0.5533333333333333\n",
      "Training accuracy: 0.5517241379310345\n",
      "Training error: 0.4482758620689655\n",
      "Test accuracy: 0.5128205128205128\n",
      "Test error: 0.4871794871794872\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "best_C = None\n",
    "best_score = 0\n",
    "#Cross validation\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(C=C, solver='lbfgs')\n",
    "    scores = cross_val_score(model, X_train, Y_train, cv=5)  # 5-fold cross-validation\n",
    "    avg_score = scores.mean()\n",
    "    \n",
    "    print(f\"C: {C}, Average Cross-Validation Score: {avg_score}\")\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_C = C\n",
    "\n",
    "print(f\"Best C: {best_C} with score: {best_score}\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "final_model = LogisticRegression(C=best_C, solver='lbfgs')\n",
    "final_model.fit(X_train, Y_train)\n",
    "Y_train_pred = final_model.predict(X_train)\n",
    "training_accuracy = accuracy_score(Y_train, Y_train_pred)\n",
    "training_error = 1 - training_accuracy\n",
    "print(f\"Training accuracy: {training_accuracy}\")\n",
    "print(f\"Training error: {training_error}\")\n",
    "Y_test_pred = final_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "test_error = 1 - test_accuracy\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test error: {test_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf22d3-b374-4eb6-8521-5863aee3ef8f",
   "metadata": {},
   "source": [
    "# 50 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21cb7871-903b-43d7-87ec-d6ee465fa96c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73, 2)\n",
      "(73,)\n",
      "(73, 2)\n",
      "(73,)\n"
     ]
    }
   ],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:2]\n",
    "Y_shuffled = X_and_Y[:,2]\n",
    "\n",
    "split_index = int(0.5 * len(X_shuffled))\n",
    "\n",
    "X_train = X_shuffled[:split_index]\n",
    "Y_train = Y_shuffled[:split_index]\n",
    "X_test = X_shuffled[split_index:]\n",
    "Y_test = Y_shuffled[split_index:]   \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d3ec82-6934-4f39-a9c1-e05448b16607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.001, Average Cross-Validation Score: 0.52\n",
      "C: 0.01, Average Cross-Validation Score: 0.52\n",
      "C: 0.1, Average Cross-Validation Score: 0.4647619047619048\n",
      "C: 1, Average Cross-Validation Score: 0.5333333333333333\n",
      "C: 10, Average Cross-Validation Score: 0.5466666666666666\n",
      "C: 100, Average Cross-Validation Score: 0.5466666666666666\n",
      "Best C: 10 with score: 0.5466666666666666\n",
      "Training accuracy: 0.547945205479452\n",
      "Training error: 0.452054794520548\n",
      "Test accuracy: 0.5068493150684932\n",
      "Test error: 0.4931506849315068\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "best_C = None\n",
    "best_score = 0\n",
    "#Cross validation\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(C=C, solver='lbfgs')\n",
    "    scores = cross_val_score(model, X_train, Y_train, cv=5)  # 5-fold cross-validation\n",
    "    avg_score = scores.mean()\n",
    "    \n",
    "    print(f\"C: {C}, Average Cross-Validation Score: {avg_score}\")\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_C = C\n",
    "\n",
    "print(f\"Best C: {best_C} with score: {best_score}\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "final_model = LogisticRegression(C=best_C, solver='lbfgs')\n",
    "final_model.fit(X_train, Y_train)\n",
    "Y_train_pred = final_model.predict(X_train)\n",
    "training_accuracy = accuracy_score(Y_train, Y_train_pred)\n",
    "training_error = 1 - training_accuracy\n",
    "print(f\"Training accuracy: {training_accuracy}\")\n",
    "print(f\"Training error: {training_error}\")\n",
    "Y_test_pred = final_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "test_error = 1 - test_accuracy\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test error: {test_error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4e9ce3-94ea-4ca9-90c1-db342dc969c0",
   "metadata": {},
   "source": [
    "# 80 training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb51af21-626d-42fc-a83a-efed8672572c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116, 2)\n",
      "(116,)\n",
      "(30, 2)\n",
      "(30,)\n"
     ]
    }
   ],
   "source": [
    "# Divide the data points into training set and test set.\n",
    "X_shuffled = X_and_Y[:,:2]\n",
    "Y_shuffled = X_and_Y[:,2]\n",
    "\n",
    "split_index = int(0.8 * len(X_shuffled))\n",
    "\n",
    "X_train = X_shuffled[:split_index]\n",
    "Y_train = Y_shuffled[:split_index]\n",
    "X_test = X_shuffled[split_index:]\n",
    "Y_test = Y_shuffled[split_index:]   \n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51235b46-5425-4e1a-a42f-da0c031edf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C: 0.001, Average Cross-Validation Score: 0.5344202898550725\n",
      "C: 0.01, Average Cross-Validation Score: 0.5344202898550725\n",
      "C: 0.1, Average Cross-Validation Score: 0.4818840579710145\n",
      "C: 1, Average Cross-Validation Score: 0.4909420289855073\n",
      "C: 10, Average Cross-Validation Score: 0.4909420289855073\n",
      "C: 100, Average Cross-Validation Score: 0.4909420289855073\n",
      "Best C: 0.001 with score: 0.5344202898550725\n",
      "Training accuracy: 0.5344827586206896\n",
      "Training error: 0.4655172413793104\n",
      "Test accuracy: 0.4666666666666667\n",
      "Test error: 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "\n",
    "best_C = None\n",
    "best_score = 0\n",
    "#Cross validation\n",
    "for C in C_values:\n",
    "    model = LogisticRegression(C=C, solver='lbfgs')\n",
    "    scores = cross_val_score(model, X_train, Y_train, cv=5)  # 5-fold cross-validation\n",
    "    avg_score = scores.mean()\n",
    "    \n",
    "    print(f\"C: {C}, Average Cross-Validation Score: {avg_score}\")\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_C = C\n",
    "\n",
    "print(f\"Best C: {best_C} with score: {best_score}\")\n",
    "from sklearn.metrics import accuracy_score\n",
    "final_model = LogisticRegression(C=best_C, solver='lbfgs')\n",
    "final_model.fit(X_train, Y_train)\n",
    "Y_train_pred = final_model.predict(X_train)\n",
    "training_accuracy = accuracy_score(Y_train, Y_train_pred)\n",
    "training_error = 1 - training_accuracy\n",
    "print(f\"Training accuracy: {training_accuracy}\")\n",
    "print(f\"Training error: {training_error}\")\n",
    "Y_test_pred = final_model.predict(X_test)\n",
    "test_accuracy = accuracy_score(Y_test, Y_test_pred)\n",
    "test_error = 1 - test_accuracy\n",
    "print(f\"Test accuracy: {test_accuracy}\")\n",
    "print(f\"Test error: {test_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3857de5-a37c-4827-b83c-9bf07cea7e5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
